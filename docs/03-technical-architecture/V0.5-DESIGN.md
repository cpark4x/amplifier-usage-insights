# V0.5 Technical Design: Validation Build

**Owner:** Chris Park  
**Contributors:** Chris Park (with AI)

**Last Updated:** 2026-02-03

---

## Summary

V0.5 is a **validation build** designed to answer one critical question: **"Can conversational insights drive behavior change in AI usage?"**

This is NOT a feature-complete release. It's a focused experiment to validate the core assumption before investing 12 weeks in the full V1 architecture.

**Build time target:** 3-6 hours (solo with AI)  
**Validation period:** 2 weeks of personal usage  
**Success criteria:** Do I check it 3+ times per week? Does it change my behavior?

---

## Table of Contents

1. [V0.5 Scope](#1-v05-scope)
2. [Simplified Architecture](#2-simplified-architecture)
3. [Data Models](#3-data-models)
4. [Technology Stack](#4-technology-stack)
5. [Module Structure](#5-module-structure)
6. [Implementation Plan](#6-implementation-plan)
7. [Success Criteria](#7-success-criteria)
8. [What We'll Learn](#8-what-well-learn)

---

## 1. V0.5 Scope

### âœ… What's IN (Must Have for Validation)

**Core Analytics:**
- âœ… Session parser - Read events.jsonl, transcript.jsonl, metadata.json
- âœ… Basic metrics calculator - Tool usage counts, session duration, turn count, delegation count
- âœ… Simple growth tracking - This week vs last week comparison
- âœ… SQLite storage - Local ~/.amplifier-usage-insights/metrics.db

**Conversational Interface:**
- âœ… Amplifier tool module - Natural language queries
- âœ… Three core queries:
  - "How am I doing this week?"
  - "What tools do I use most?"
  - "Am I improving?"

**Rule-Based Tips:**
- âœ… 5 simple patterns:
  1. High bash usage â†’ suggest grep/glob
  2. Low delegation â†’ suggest breaking down problems
  3. High error rate â†’ suggest asking for alternatives
  4. Tool diversity declining â†’ suggest exploring new tools
  5. Session duration increasing â†’ suggest smaller tasks

**Testing:**
- âœ… Unit tests for parser and metrics (60%+ coverage)
- âœ… Integration test with real session data

---

### âŒ What's OUT (Defer to V1.0)

**Deferred to V1.0:**
- âŒ Web dashboard (Vue.js) - Validate conversational first
- âŒ LLM-based tips - Rule-based is enough for validation
- âŒ Advanced analytics - Sophisticated growth models, pattern detection
- âŒ File watcher - Manual refresh OK for validation
- âŒ Background service - Run on-demand only
- âŒ API server (FastAPI) - Not needed without dashboard
- âŒ Complex visualizations - Text output is sufficient
- âŒ Tip feedback system - Track manually during validation

**Deferred to V2+:**
- âŒ Team insights
- âŒ Manager insights
- âŒ Multi-user support

---

## 2. Simplified Architecture

### Component View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CONVERSATIONAL INTERFACE             â”‚
â”‚         (Amplifier Tool Module)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Insights Engine    â”‚
      â”‚  (Query + Format)    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Analytics Core     â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚  â€¢ Metrics Calc      â”‚
      â”‚  â€¢ Growth Compare    â”‚
      â”‚  â€¢ Rule-Based Tips   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   SQLite Database    â”‚
      â”‚  (Local Storage)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Session Parser     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Amplifier Sessions  â”‚
      â”‚ ~/.amplifier/projectsâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow (Simplified)

```
1. User runs: amplifier-insights refresh
   â†“
2. Session Parser scans ~/.amplifier/projects/
   â†“
3. For each session: Extract metrics (tools, duration, turns, delegations)
   â†“
4. Metrics Calculator computes session metrics
   â†“
5. Store in SQLite (sessions + session_metrics tables)
   â†“
6. Compute weekly aggregations (user_metrics table)
   â†“
7. Compare to previous week (growth indicators)
   â†“
8. Generate rule-based tips based on patterns
   â†“
9. User asks in Amplifier: "How am I doing?"
   â†“
10. Insights Engine queries SQLite + formats response
```

### Components (6 total, down from 10 in V1)

| Component | Responsibility | Input | Output |
|-----------|---------------|-------|--------|
| **Session Parser** | Parse Amplifier session files | Session directory paths | Structured session data |
| **Metrics Calculator** | Compute session metrics | Session data | SessionMetrics |
| **Growth Comparator** | Compare periods | Current + previous week metrics | Growth indicators |
| **Rule-Based Tips** | Pattern matching for tips | Session metrics | Actionable tips |
| **Insights Engine** | Query interface | Natural language queries | Formatted responses |
| **Conversational Tool** | Amplifier integration | User queries | Tool responses |

---

## 3. Data Models

### Simplified Data Models (3 core entities)

#### **Session** (Minimal - What We Parse)

```python
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Session:
    """Minimal session data for V0.5."""
    
    # Identity
    session_id: str
    project_path: str
    
    # Timing
    started_at: datetime
    ended_at: datetime
    duration_seconds: int
    
    # Basic counts
    turn_count: int
    tool_call_count: int
    delegation_count: int
    error_count: int
    
    # Tool usage (simple dict)
    tool_counts: dict[str, int]  # {"bash": 12, "read_file": 8}
    
    # Metadata
    model_used: str
    status: str  # "completed", "abandoned"
```

#### **WeeklyMetrics** (Aggregated by Week)

```python
@dataclass
class WeeklyMetrics:
    """Aggregated metrics for one week."""
    
    # Identity
    user_id: str  # "local" for V0.5
    week_start: datetime  # Monday of the week
    
    # Volume
    session_count: int
    total_duration_seconds: int
    total_turns: int
    total_tool_calls: int
    total_delegations: int
    total_errors: int
    
    # Tool usage
    unique_tools: int
    tool_counts: dict[str, int]  # Aggregated across week
    top_5_tools: list[str]
    
    # Derived metrics
    avg_session_duration: float
    avg_turns_per_session: float
    delegation_ratio: float  # delegations / total_sessions
    error_rate: float  # errors / tool_calls
    
    # Growth (compare to previous week)
    sessions_change_pct: float | None  # +15% or -10%
    tools_change_pct: float | None
    delegation_change_pct: float | None
    error_change_pct: float | None
```

#### **RuleTip** (Simple Pattern-Based Tips)

```python
@dataclass
class RuleTip:
    """Rule-based actionable tip."""
    
    rule_id: str  # "high_bash_usage", "low_delegation", etc.
    category: str  # "tool_usage", "delegation", "error_handling"
    priority: str  # "high", "medium", "low"
    
    # The tip
    observation: str
    recommendation: str
    expected_benefit: str
    
    # When generated
    generated_at: datetime
    based_on_week: datetime
```

---

## 4. Technology Stack

### Core Application (Python Only)

```toml
[project]
name = "amplifier-usage-insights"
version = "0.5.0"  # Validation build
requires-python = ">=3.11"

dependencies = [
    # No heavy dependencies for V0.5
    "python-dateutil>=2.8.0",   # Date parsing
    "typer>=0.9.0",              # CLI framework
    "rich>=13.0.0",              # Beautiful terminal output
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.2.0",
    "pyright>=1.1.0",
]
```

**Key Simplifications:**
- âŒ No Polars (use Python stdlib + SQLite for simple aggregations)
- âŒ No FastAPI (no API server needed)
- âŒ No LiteLLM (no LLM tips in V0.5)
- âŒ No Watchdog (manual refresh)
- âœ… Keep Typer (beautiful CLI)
- âœ… Keep Rich (great terminal output)
- âœ… SQLite (built-in, no installation)

---

## 5. Module Structure

### Directory Layout (Simplified)

```
amplifier-usage-insights/
â”œâ”€â”€ pyproject.toml               # Python package config
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/                        # Vision docs (already created)
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ amplifier_usage_insights/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __main__.py          # CLI entry point
â”‚       â”‚
â”‚       â”œâ”€â”€ parser.py            # ðŸŸ¢ Session parser (single file)
â”‚       â”œâ”€â”€ metrics.py           # ðŸŸ¢ Metrics calculation (single file)
â”‚       â”œâ”€â”€ growth.py            # ðŸŸ¢ Growth comparison (single file)
â”‚       â”œâ”€â”€ tips.py              # ðŸŸ¢ Rule-based tips (single file)
â”‚       â”œâ”€â”€ storage.py           # ðŸŸ¢ SQLite operations (single file)
â”‚       â”‚
â”‚       â”œâ”€â”€ insights.py          # ðŸŸ¢ Query engine + formatters (single file)
â”‚       â”œâ”€â”€ cli.py               # ðŸŸ¢ CLI commands (single file)
â”‚       â”‚
â”‚       â””â”€â”€ schema.sql           # ðŸŸ¢ Database schema
â”‚
â”œâ”€â”€ bundles/                     # Amplifier bundle
â”‚   â””â”€â”€ usage-insights/
â”‚       â”œâ”€â”€ bundle.yaml
â”‚       â”œâ”€â”€ tools.yaml           # Tool definition
â”‚       â””â”€â”€ contexts/
â”‚           â””â”€â”€ instructions.md
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_parser.py
    â”œâ”€â”€ test_metrics.py
    â”œâ”€â”€ test_growth.py
    â”œâ”€â”€ test_insights.py
    â””â”€â”€ fixtures/
        â””â”€â”€ sample_sessions/     # Mock Amplifier sessions
```

**Architecture Note:** V0.5 uses **single-file modules** instead of packages. Each responsibility is one file. This is intentional for speed and simplicity. Refactor to packages in V1.0 if V0.5 validates.

---

## 6. Implementation Plan

### Build Order (Critical Path)

#### Step 1: Project Setup (30 min)
```bash
# Create project structure
mkdir -p src/amplifier_usage_insights
mkdir -p bundles/usage-insights/contexts
mkdir -p tests/fixtures/sample_sessions

# Create pyproject.toml
# Create schema.sql
# Initialize git
```

**Output:** Empty project structure ready for code

---

#### Step 2: Database Schema (15 min)

**File:** `src/amplifier_usage_insights/schema.sql`

```sql
-- V0.5 Schema (Minimal - 3 tables)

-- Raw sessions (parsed from Amplifier)
CREATE TABLE IF NOT EXISTS sessions (
    session_id TEXT PRIMARY KEY,
    project_path TEXT NOT NULL,
    started_at TIMESTAMP NOT NULL,
    ended_at TIMESTAMP NOT NULL,
    duration_seconds INTEGER NOT NULL,
    turn_count INTEGER NOT NULL,
    tool_call_count INTEGER NOT NULL,
    delegation_count INTEGER NOT NULL,
    error_count INTEGER NOT NULL,
    model_used TEXT,
    status TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tool usage per session
CREATE TABLE IF NOT EXISTS session_tools (
    session_id TEXT NOT NULL,
    tool_name TEXT NOT NULL,
    call_count INTEGER NOT NULL,
    PRIMARY KEY (session_id, tool_name),
    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);

-- Weekly aggregations
CREATE TABLE IF NOT EXISTS weekly_metrics (
    user_id TEXT NOT NULL,
    week_start TIMESTAMP NOT NULL,
    session_count INTEGER NOT NULL,
    total_duration_seconds INTEGER NOT NULL,
    total_turns INTEGER NOT NULL,
    total_tool_calls INTEGER NOT NULL,
    total_delegations INTEGER NOT NULL,
    total_errors INTEGER NOT NULL,
    unique_tools INTEGER NOT NULL,
    top_5_tools TEXT NOT NULL,  -- JSON array
    avg_session_duration REAL NOT NULL,
    delegation_ratio REAL NOT NULL,
    error_rate REAL NOT NULL,
    sessions_change_pct REAL,
    tools_change_pct REAL,
    delegation_change_pct REAL,
    error_change_pct REAL,
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (user_id, week_start)
);

-- Indexes for common queries
CREATE INDEX IF NOT EXISTS idx_sessions_started_at ON sessions(started_at);
CREATE INDEX IF NOT EXISTS idx_weekly_metrics_week ON weekly_metrics(week_start);
```

**Output:** Database schema ready for storage.py

---

#### Step 3: Session Parser (1-1.5 hours)

**File:** `src/amplifier_usage_insights/parser.py`

**Contract:**

```python
from pathlib import Path
from datetime import datetime

class SessionParser:
    """Parse Amplifier session files into structured data."""
    
    def parse_session(self, session_dir: Path) -> Session:
        """
        Parse a single Amplifier session directory.
        
        Input: Path to session dir containing:
            - events.jsonl (tool calls, errors)
            - transcript.jsonl (turns)
            - metadata.json (session info)
        
        Output: Session object with:
            - session_id, project_path
            - started_at, ended_at, duration_seconds
            - turn_count, tool_call_count, delegation_count, error_count
            - tool_counts: {"bash": 12, "delegate": 8}
            - model_used, status
        
        Errors:
            - FileNotFoundError if session files missing
            - JSONDecodeError if files corrupted
        """
        pass
    
    def find_sessions(self, projects_dir: Path) -> list[Path]:
        """
        Find all session directories in ~/.amplifier/projects/
        
        Returns: List of session directory paths
        """
        pass
```

**Key Implementation Notes:**

1. **events.jsonl parsing:**
   - Count tool calls by tool name
   - Count delegation calls (tool="delegate")
   - Count errors (look for error events)
   - Extract session_id from first event

2. **transcript.jsonl parsing:**
   - Count turns (user/assistant message pairs)
   - Get started_at from first message
   - Get ended_at from last message

3. **metadata.json parsing:**
   - Get project_path, model_used, status

**Edge Cases:**
- Empty events.jsonl â†’ 0 tool calls
- Missing transcript.jsonl â†’ Use events for timing
- Incomplete session (no end) â†’ status = "abandoned"

---

#### Step 4: SQLite Storage (45 min)

**File:** `src/amplifier_usage_insights/storage.py`

**Contract:**

```python
from pathlib import Path
import sqlite3

class MetricsDB:
    """Simple SQLite operations for V0.5."""
    
    def __init__(self, db_path: Path):
        """Initialize DB connection, create schema if needed."""
        self.db_path = db_path
        self._ensure_schema()
    
    def save_session(self, session: Session) -> None:
        """
        Insert or replace session in database.
        
        Inserts into:
        - sessions table
        - session_tools table
        """
        pass
    
    def get_weekly_metrics(self, week_start: datetime) -> WeeklyMetrics | None:
        """Get metrics for a specific week."""
        pass
    
    def compute_weekly_metrics(self, week_start: datetime) -> WeeklyMetrics:
        """
        Aggregate all sessions in a week into WeeklyMetrics.
        
        Queries sessions WHERE started_at BETWEEN week_start AND week_end,
        aggregates counts, computes averages, compares to previous week.
        """
        pass
    
    def get_all_sessions(self) -> list[Session]:
        """Get all sessions (for debugging)."""
        pass
```

**Key Implementation Notes:**

1. **Schema initialization:**
   - Read schema.sql on first run
   - Execute CREATE TABLE statements
   - SQLite built-in, no migrations needed for V0.5

2. **Save session:**
   - INSERT OR REPLACE into sessions
   - DELETE old session_tools, INSERT new ones
   - Single transaction

3. **Compute weekly metrics:**
   - SQL aggregations (SUM, AVG, COUNT)
   - Get previous week's metrics for comparison
   - Calculate % change

---

#### Step 5: Metrics Calculator (45 min)

**File:** `src/amplifier_usage_insights/metrics.py`

**Contract:**

```python
from datetime import datetime, timedelta

def get_week_start(dt: datetime) -> datetime:
    """Get Monday 00:00:00 for the week containing dt."""
    pass

def calculate_growth(current: WeeklyMetrics, previous: WeeklyMetrics | None) -> dict:
    """
    Calculate growth indicators.
    
    Returns:
        {
            "sessions_change_pct": 15.0,  # +15%
            "tools_change_pct": 0.0,
            "delegation_change_pct": -5.0,  # -5%
            "error_change_pct": 10.0,
            "trend": "improving",  # or "declining", "stable"
        }
    """
    pass

def calculate_weekly_metrics(db: MetricsDB, week_start: datetime) -> WeeklyMetrics:
    """
    Aggregate sessions in week into WeeklyMetrics.
    
    Queries all sessions where started_at in [week_start, week_start + 7 days),
    computes counts, averages, ratios, and compares to previous week.
    """
    pass
```

**Key Calculations:**

1. **Delegation ratio:** `delegation_count / session_count`
2. **Error rate:** `total_errors / total_tool_calls`
3. **Avg session duration:** `total_duration / session_count`
4. **Tool diversity:** `unique_tools` (simple count for V0.5, not Shannon entropy)
5. **Change %:** `((current - previous) / previous) * 100`

---

#### Step 6: Rule-Based Tips (30 min)

**File:** `src/amplifier_usage_insights/tips.py`

**Contract:**

```python
def generate_tips(current_week: WeeklyMetrics, previous_week: WeeklyMetrics | None) -> list[RuleTip]:
    """
    Generate tips based on simple pattern matching.
    
    Rules:
    1. If bash usage > 30% of total tools â†’ suggest grep/glob
    2. If delegation_ratio < 0.3 â†’ suggest breaking down problems
    3. If error_rate > 0.15 â†’ suggest asking for alternatives
    4. If unique_tools declining â†’ suggest exploring new tools
    5. If avg_session_duration > 60min â†’ suggest smaller tasks
    
    Returns: List of 0-5 tips (only triggered rules)
    """
    pass
```

**Example Rule Implementation:**

```python
def rule_high_bash_usage(metrics: WeeklyMetrics) -> RuleTip | None:
    """Suggest grep/glob if bash is overused."""
    
    bash_count = metrics.tool_counts.get("bash", 0)
    total_calls = metrics.total_tool_calls
    
    if total_calls == 0:
        return None
    
    bash_pct = bash_count / total_calls
    
    if bash_pct > 0.30:  # >30% bash usage
        return RuleTip(
            rule_id="high_bash_usage",
            category="tool_usage",
            priority="medium",
            observation=f"You use bash {bash_pct:.0%} of the time ({bash_count} calls this week)",
            recommendation="Try using grep for searching files and glob for finding files instead of bash commands",
            expected_benefit="30% faster file operations with specialized tools",
            generated_at=datetime.now(),
            based_on_week=metrics.week_start
        )
    
    return None
```

---

#### Step 7: Insights Engine (1 hour)

**File:** `src/amplifier_usage_insights/insights.py`

**Contract:**

```python
class InsightsEngine:
    """Query interface for V0.5 conversational tool."""
    
    def __init__(self, db_path: Path):
        self.db = MetricsDB(db_path)
    
    def query_weekly_summary(self, time_range: str = "this_week") -> dict:
        """
        Get weekly summary for conversational response.
        
        Returns:
            {
                "summary": "12 sessions this week, up 50% from last week",
                "metrics": {
                    "sessions": {"count": 12, "change": "+50%"},
                    "tools": {"unique": 8, "top_5": ["bash", "delegate", ...]},
                    "effectiveness": {"avg_duration": "25min", "delegation_ratio": 0.4}
                },
                "growth": {
                    "trend": "improving",
                    "strongest_area": "delegation",
                    "areas_to_improve": ["error_handling"]
                },
                "tips": [...]  # RuleTip objects
            }
        """
        pass
    
    def query_tool_usage(self) -> dict:
        """Get tool usage breakdown."""
        pass
    
    def query_growth(self) -> dict:
        """Get growth indicators."""
        pass
```

**Conversational Formatter:**

```python
def format_conversational_response(data: dict) -> str:
    """
    Format insights data for natural language response.
    
    Input: query_weekly_summary() output
    
    Output: Human-readable string like:
    
    "You're showing strong growth! ðŸš€
    
    This Week vs Last Week:
    â€¢ 12 sessions (+50% from 8 last week)
    â€¢ 8 different tools used (up from 6)
    â€¢ Delegation ratio: 40% (you're breaking down problems well!)
    â€¢ Error rate: 12% (slightly higher than last week)
    
    Growth Area: Error Handling
    Your error rate increased 10% this week. When you hit errors, 
    try asking for alternative approaches instead of retrying the same path.
    
    Top Tools This Week:
    1. bash (45 calls) - consider using grep/glob for file operations
    2. delegate (32 calls) - great job breaking down problems!
    3. read_file (28 calls)
    4. edit_file (18 calls)
    5. grep (12 calls)
    
    Overall trend: Improving ðŸ“ˆ (3 weeks of growth)"
    """
    pass
```

---

#### Step 8: CLI Commands (30 min)

**File:** `src/amplifier_usage_insights/cli.py`

**Commands:**

```python
import typer
from rich.console import Console

app = typer.Typer()
console = Console()

@app.command()
def init():
    """Initialize the insights database."""
    # Create ~/.amplifier-usage-insights/metrics.db
    # Run schema.sql
    console.print("[green]âœ“[/green] Database initialized")

@app.command()
def refresh():
    """Scan sessions and update metrics."""
    # Find all sessions in ~/.amplifier/projects/
    # Parse each session
    # Save to database
    # Compute weekly metrics
    console.print(f"[green]âœ“[/green] Processed {count} sessions")

@app.command()
def status():
    """Show current metrics summary."""
    # Query this week's metrics
    # Print summary to terminal
    pass

@app.command()
def show(
    query: str = typer.Argument("weekly"),
):
    """
    Show insights.
    
    Examples:
        amplifier-insights show weekly
        amplifier-insights show tools
        amplifier-insights show growth
    """
    pass
```

---

#### Step 9: Amplifier Tool Module (45 min)

**File:** `bundles/usage-insights/tools.yaml`

```yaml
tools:
  - name: get_usage_insights
    description: |
      Get insights about your AI collaboration effectiveness.
      
      Ask questions like:
      - "How am I doing this week?"
      - "What tools do I use most?"
      - "Am I improving?"
      - "Show my growth"
    
    implementation:
      module: amplifier_usage_insights.insights
      function: get_personal_insights
    
    parameters:
      query:
        type: string
        description: What you want to know about your AI usage
        required: false
        default: "How am I doing this week?"
```

**File:** `bundles/usage-insights/bundle.yaml`

```yaml
name: usage-insights
version: 0.5.0
description: Conversational insights about your AI collaboration effectiveness

tools:
  - tools.yaml

contexts:
  - path: contexts/instructions.md
    expose_as: usage-insights-guide
```

**File:** `src/amplifier_usage_insights/insights.py` (add function)

```python
def get_personal_insights(query: str = "How am I doing this week?") -> dict:
    """
    Tool implementation for Amplifier.
    
    Called when user asks about their AI usage in any Amplifier session.
    
    Returns: Structured data that Amplifier formats into conversational response
    """
    
    # Initialize engine
    db_path = Path.home() / ".amplifier-usage-insights" / "metrics.db"
    engine = InsightsEngine(db_path)
    
    # Simple query routing
    if "tool" in query.lower():
        data = engine.query_tool_usage()
    elif "grow" in query.lower() or "improv" in query.lower():
        data = engine.query_growth()
    else:
        data = engine.query_weekly_summary()
    
    # Format for conversational response
    return {
        "response": format_conversational_response(data),
        "data": data  # Include structured data for Amplifier to use
    }
```

---

#### Step 10: Unit Tests (1 hour)

**Files:**
- `tests/test_parser.py` - Test session parsing with fixtures
- `tests/test_metrics.py` - Test calculations
- `tests/test_growth.py` - Test growth comparisons
- `tests/test_insights.py` - Test query engine

**Fixtures:**

Create 3 realistic mock sessions:

```
tests/fixtures/sample_sessions/
â”œâ”€â”€ simple_session/
â”‚   â”œâ”€â”€ events.jsonl
â”‚   â”œâ”€â”€ transcript.jsonl
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ delegation_session/
â”‚   â””â”€â”€ ...
â””â”€â”€ error_session/
    â””â”€â”€ ...
```

**Critical Tests:**

```python
def test_parse_simple_session():
    """Parse a basic session correctly."""
    parser = SessionParser()
    session = parser.parse_session(Path("tests/fixtures/sample_sessions/simple_session"))
    
    assert session.session_id is not None
    assert session.turn_count > 0
    assert session.tool_call_count > 0
    assert len(session.tool_counts) > 0

def test_weekly_metrics_calculation():
    """Aggregate sessions into weekly metrics."""
    # Create 5 sessions in current week
    # Compute weekly metrics
    # Verify counts and averages
    pass

def test_growth_with_no_previous_week():
    """Handle first week of usage (no comparison)."""
    growth = calculate_growth(current_week, previous_week=None)
    assert growth["sessions_change_pct"] is None
    assert growth["trend"] == "stable"
```

---

## 7. Success Criteria

### V0.5 Validation Metrics

**Build Success:**
- âœ… All tests pass (pytest)
- âœ… Can parse 10 real Amplifier sessions without errors
- âœ… Weekly metrics compute correctly
- âœ… Conversational tool works in Amplifier
- âœ… Response time <2 seconds

**Usage Validation (2 weeks):**
- âœ… I check insights 3+ times per week
- âœ… I find at least 1 tip actionable
- âœ… I change my behavior based on insights
- âœ… I want more detailed analytics (validates building dashboard)

**Failure Criteria (Stop Signal):**
- âŒ I check less than 1x per week
- âŒ Tips feel generic/useless
- âŒ No behavior change
- âŒ Insights don't feel valuable

**If V0.5 fails validation:** Don't build V1.0. Pivot or abandon.

---

## 8. What We'll Learn

### Key Questions V0.5 Answers

1. **Will I actually use conversational insights?**
   - Measure: How often do I query in 2 weeks?
   - Learn: Is conversational interface valuable or gimmick?

2. **Which metrics matter?**
   - Measure: Which queries do I ask most?
   - Learn: Focus V1.0 on high-value metrics

3. **Do tips drive behavior change?**
   - Measure: Do I follow tip suggestions?
   - Learn: Are rule-based tips sufficient or need LLM?

4. **Is weekly granularity right?**
   - Measure: Do I want daily/monthly views?
   - Learn: What time ranges to support in V1.0

5. **Is growth tracking motivating?**
   - Measure: Do I check "am I improving?" regularly?
   - Learn: How prominent should growth be in V1.0

6. **Do I need the dashboard?**
   - Measure: Do I wish for visualizations?
   - Learn: Dashboard priority for V1.0

---

## Implementation Timeline

### Total Time: 3-6 hours

| Step | Component | Time | Dependencies |
|------|-----------|------|--------------|
| 1 | Project setup | 30 min | None |
| 2 | Database schema | 15 min | Step 1 |
| 3 | Session parser | 1.5 hrs | Step 1 |
| 4 | SQLite storage | 45 min | Steps 2, 3 |
| 5 | Metrics calculator | 45 min | Step 4 |
| 6 | Rule-based tips | 30 min | Step 5 |
| 7 | Insights engine | 1 hr | Steps 5, 6 |
| 8 | CLI commands | 30 min | Step 7 |
| 9 | Amplifier tool | 45 min | Step 7 |
| 10 | Unit tests | 1 hr | All above |

**Critical Path:** Steps 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 7 â†’ 9

**Parallel Work:**
- Step 6 (tips) can be built alongside Step 7
- Step 8 (CLI) can be built alongside Step 9
- Step 10 (tests) throughout

---

## File-by-File Specification

### 1. `pyproject.toml`

```toml
[project]
name = "amplifier-usage-insights"
version = "0.5.0"
description = "Conversational insights about your AI collaboration effectiveness (validation build)"
authors = [{name = "Chris Park"}]
requires-python = ">=3.11"
readme = "README.md"
license = {text = "MIT"}

dependencies = [
    "python-dateutil>=2.8.0",
    "typer>=0.9.0",
    "rich>=13.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.2.0",
    "pyright>=1.1.0",
]

[project.scripts]
amplifier-insights = "amplifier_usage_insights.cli:app"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
line-length = 100
target-version = "py311"

[tool.pyright]
typeCheckingMode = "strict"
```

---

### 2. `src/amplifier_usage_insights/__init__.py`

```python
"""Amplifier Usage Insights - V0.5 Validation Build."""

__version__ = "0.5.0"

from .parser import SessionParser, Session
from .metrics import WeeklyMetrics, calculate_growth, get_week_start
from .storage import MetricsDB
from .tips import RuleTip, generate_tips
from .insights import InsightsEngine, get_personal_insights

__all__ = [
    "SessionParser",
    "Session",
    "WeeklyMetrics",
    "MetricsDB",
    "RuleTip",
    "InsightsEngine",
    "get_personal_insights",
    "calculate_growth",
    "generate_tips",
    "get_week_start",
]
```

---

### 3. `src/amplifier_usage_insights/__main__.py`

```python
"""CLI entry point."""

from .cli import app

if __name__ == "__main__":
    app()
```

---

### 4. Data Flow Example

**Scenario:** User asks "How am I doing this week?" in Amplifier

```python
# 1. Amplifier calls tool
result = get_personal_insights("How am I doing this week?")

# 2. InsightsEngine initializes
engine = InsightsEngine(db_path="~/.amplifier-usage-insights/metrics.db")

# 3. Query weekly summary
data = engine.query_weekly_summary(time_range="this_week")

# 4. Engine calls storage
current_week = db.get_weekly_metrics(get_week_start(datetime.now()))
previous_week = db.get_weekly_metrics(get_week_start(datetime.now() - timedelta(days=7)))

# 5. Calculate growth
growth = calculate_growth(current_week, previous_week)

# 6. Generate tips
tips = generate_tips(current_week, previous_week)

# 7. Format response
response = format_conversational_response({
    "current": current_week,
    "growth": growth,
    "tips": tips
})

# 8. Return to Amplifier
return {"response": response, "data": {...}}
```

---

## Database Initialization

### First Run Flow

```bash
# User installs
pip install -e ~/Projects/amplifier-usage-insights

# Initialize database
amplifier-insights init
â†’ Creates ~/.amplifier-usage-insights/
â†’ Creates metrics.db
â†’ Runs schema.sql

# Scan existing sessions
amplifier-insights refresh
â†’ Finds sessions in ~/.amplifier/projects/
â†’ Parses each session
â†’ Saves to database
â†’ Computes weekly metrics
â†’ Prints: "âœ“ Processed 47 sessions across 6 weeks"

# Check status
amplifier-insights status
â†’ Shows current week summary in terminal

# Install bundle
ln -s ~/Projects/amplifier-usage-insights/bundles/usage-insights ~/.amplifier/bundles/

# Use in Amplifier
[In any Amplifier session]
You: "How am I doing this week?"
â†’ Tool calls get_usage_insights()
â†’ Returns formatted insights
```

---

## Key V0.5 Constraints

### What Makes V0.5 "Validation Build"

1. **No background service** - Manual `refresh` command
2. **No web UI** - Terminal + conversational only
3. **No real-time updates** - Batch refresh when needed
4. **Simple metrics** - Counts and ratios, no sophisticated analysis
5. **Rule-based tips** - Pattern matching, no LLM
6. **Single user** - "local" user_id hardcoded
7. **Local only** - No cloud, no API server
8. **Basic error handling** - Log errors, don't crash

### What V0.5 Validates

- âœ… **Core value:** Do conversational insights help?
- âœ… **Engagement:** Will I check it regularly?
- âœ… **Behavior change:** Do tips drive improvement?
- âœ… **Metric relevance:** Which metrics matter?
- âœ… **Build feasibility:** Can we build this solo with AI?

### What V0.5 Doesn't Validate

- âŒ Dashboard usefulness (not built yet)
- âŒ LLM tip quality (using rules)
- âŒ Team features (single user)
- âŒ Real-time updates (manual refresh)

---

## Risk Mitigation

### Risk 1: Can't Parse Real Sessions

**Mitigation:**
- Test with 3 diverse real sessions during development
- Handle edge cases (missing files, corrupt JSON)
- Log errors, continue processing other sessions

### Risk 2: Metrics Aren't Meaningful

**Mitigation:**
- Start with simplest metrics (counts)
- Add complexity only if simple metrics validate
- Build feedback into validation (note which metrics I ignore)

### Risk 3: Build Takes Too Long

**Mitigation:**
- Use AI agents (modular-builder, bug-hunter)
- Copy patterns from V1 architecture
- Skip polish - working > beautiful

### Risk 4: I Don't Use It

**Mitigation:**
- Make query friction as low as possible
- Print reminder after Amplifier sessions: "Check your insights with get_usage_insights()"
- Set calendar reminder to check weekly

---

## V0.5 to V1.0 Migration Path

**If V0.5 validates successfully:**

1. **Keep:**
   - Session parser (working, tested)
   - Database schema (extend, don't replace)
   - Metrics calculator (refine based on learnings)
   - Conversational tool (proven valuable)

2. **Add:**
   - FastAPI backend
   - Vue.js dashboard
   - LLM tips generation
   - File watcher (real-time updates)
   - Advanced analytics
   - Background service

3. **Refactor:**
   - Single files â†’ Package structure (parser.py â†’ ingestion/)
   - Hardcoded "local" â†’ User ID system
   - Simple metrics â†’ Sophisticated analysis

**If V0.5 fails validation:**

- Analyze: Which parts worked? Which didn't?
- Pivot: Maybe just a CLI tool, not conversational?
- Abandon: If no engagement, maybe problem isn't real

---

## Next Steps

### To Start Building V0.5

**Option A: Build Everything at Once**
```bash
# Delegate to modular-builder with this design doc
# Build all 10 steps in one session
# Test with real sessions
```

**Option B: Build Incrementally (Recommended)**
```bash
# Phase 1: Parser + Storage (Steps 1-4)
#  â†’ Validate we can parse sessions correctly
# Phase 2: Metrics + Tips (Steps 5-6)
#  â†’ Validate calculations make sense
# Phase 3: Interface (Steps 7-9)
#  â†’ Validate conversational UX
# Phase 4: Tests (Step 10)
#  â†’ Ensure quality
```

**Option C: Minimal Viable First**
```bash
# Build ONLY: parser + storage + simple CLI
# Skip: growth tracking, tips, Amplifier tool
# Goal: "Can I extract basic metrics?"
# Timeline: 1-2 hours
```

---

## Implementation Specifications

### Session Parser Specification

**File:** `src/amplifier_usage_insights/parser.py`

**Dependencies:**
```python
import json
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
```

**Session Discovery:**
```python
def find_sessions(projects_dir: Path = Path.home() / ".amplifier" / "projects") -> list[Path]:
    """
    Find all session directories.
    
    Structure:
        ~/.amplifier/projects/
        â”œâ”€â”€ project-a/
        â”‚   â””â”€â”€ sessions/
        â”‚       â”œâ”€â”€ session-id-1/
        â”‚       â””â”€â”€ session-id-2/
        â””â”€â”€ project-b/
            â””â”€â”€ sessions/
                â””â”€â”€ session-id-3/
    
    Returns: [session-id-1/, session-id-2/, session-id-3/]
    """
    sessions = []
    
    for project_dir in projects_dir.iterdir():
        if not project_dir.is_dir():
            continue
        
        sessions_dir = project_dir / "sessions"
        if sessions_dir.exists():
            for session_dir in sessions_dir.iterdir():
                if session_dir.is_dir():
                    sessions.append(session_dir)
    
    return sessions
```

**events.jsonl Parsing:**
```python
def parse_events(events_file: Path) -> dict:
    """
    Parse events.jsonl to extract tool usage.
    
    Returns:
        {
            "tool_counts": {"bash": 12, "delegate": 8},
            "delegation_count": 8,
            "error_count": 3,
            "session_id": "abc123"
        }
    """
    tool_counts = {}
    delegation_count = 0
    error_count = 0
    session_id = None
    
    with open(events_file) as f:
        for line in f:
            event = json.loads(line)
            
            # Get session ID from first event
            if session_id is None and "session_id" in event:
                session_id = event["session_id"]
            
            # Count tool calls
            if event.get("type") == "tool_call":
                tool_name = event.get("tool", "unknown")
                tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1
                
                if tool_name == "delegate":
                    delegation_count += 1
            
            # Count errors
            if event.get("type") == "error" or event.get("status") == "error":
                error_count += 1
    
    return {
        "tool_counts": tool_counts,
        "delegation_count": delegation_count,
        "error_count": error_count,
        "session_id": session_id
    }
```

**transcript.jsonl Parsing:**
```python
def parse_transcript(transcript_file: Path) -> dict:
    """
    Parse transcript.jsonl to extract timing and turn count.
    
    Returns:
        {
            "turn_count": 12,
            "started_at": datetime(...),
            "ended_at": datetime(...)
        }
    """
    turns = 0
    started_at = None
    ended_at = None
    
    with open(transcript_file) as f:
        for line in f:
            message = json.loads(line)
            
            timestamp = datetime.fromisoformat(message.get("timestamp", ""))
            
            if started_at is None:
                started_at = timestamp
            ended_at = timestamp
            
            if message.get("role") == "user":
                turns += 1
    
    return {
        "turn_count": turns,
        "started_at": started_at,
        "ended_at": ended_at
    }
```

---

### Metrics Calculator Specification

**File:** `src/amplifier_usage_insights/metrics.py`

**Weekly Aggregation Logic:**

```python
def calculate_weekly_metrics(db: MetricsDB, week_start: datetime) -> WeeklyMetrics:
    """
    Aggregate all sessions in week into WeeklyMetrics.
    
    SQL Query:
        SELECT 
            COUNT(*) as session_count,
            SUM(duration_seconds) as total_duration,
            SUM(turn_count) as total_turns,
            SUM(tool_call_count) as total_tool_calls,
            SUM(delegation_count) as total_delegations,
            SUM(error_count) as total_errors,
            AVG(duration_seconds) as avg_session_duration
        FROM sessions
        WHERE started_at >= ? AND started_at < ?
    
    Tool Aggregation:
        SELECT tool_name, SUM(call_count)
        FROM session_tools
        WHERE session_id IN (sessions this week)
        GROUP BY tool_name
        ORDER BY SUM(call_count) DESC
    
    Growth Comparison:
        Get previous week's metrics
        Calculate % change for each metric
    """
    pass
```

**Growth Calculation:**

```python
def calculate_growth(current: WeeklyMetrics, previous: WeeklyMetrics | None) -> dict:
    """
    Compare current week to previous week.
    
    Returns:
        {
            "sessions_change_pct": 15.0,
            "tools_change_pct": 0.0,
            "delegation_change_pct": -5.0,
            "error_change_pct": 10.0,
            "trend": "improving"  # Based on weighted score
        }
    
    Trend Logic:
        improving: sessions up, delegation up, errors down, tools up
        declining: sessions down, delegation down, errors up
        stable: mixed signals or <10% changes
    """
    
    if previous is None:
        return {
            "sessions_change_pct": None,
            "tools_change_pct": None,
            "delegation_change_pct": None,
            "error_change_pct": None,
            "trend": "stable"
        }
    
    # Calculate % changes
    sessions_change = ((current.session_count - previous.session_count) / previous.session_count * 100) if previous.session_count > 0 else 0
    
    # Similar for other metrics...
    
    # Determine trend (simple scoring)
    score = 0
    if sessions_change > 10: score += 1
    if current.delegation_ratio > previous.delegation_ratio: score += 1
    if current.error_rate < previous.error_rate: score += 1
    if current.unique_tools > previous.unique_tools: score += 1
    
    trend = "improving" if score >= 3 else "declining" if score <= 1 else "stable"
    
    return {
        "sessions_change_pct": sessions_change,
        # ... other changes
        "trend": trend
    }
```

---

## Testing Strategy (Simplified)

### Test Coverage Target: 60%+

Focus on critical paths only for V0.5.

### Must Test:
- âœ… Session parsing (all 3 file types)
- âœ… Metrics calculation (weekly aggregation)
- âœ… Growth comparison (with and without previous week)
- âœ… Rule-based tips (all 5 rules)
- âœ… Insights engine (query routing)

### Can Skip:
- âŒ Edge cases (unusual session formats)
- âŒ Performance tests (not critical for V0.5)
- âŒ CLI commands (manual testing OK)

### Fixture Strategy

**Create 1 realistic fixture:**
```
tests/fixtures/sample_week/
â”œâ”€â”€ session-1/  # Simple session
â”œâ”€â”€ session-2/  # Delegation-heavy
â””â”€â”€ session-3/  # Error-heavy
```

**Use for:**
- Integration test: Parse all 3 sessions â†’ Compute weekly metrics â†’ Generate tips
- Validate calculations manually against expected values

---

## Installation & Usage

### Installation (V0.5)

```bash
# Clone or navigate to project
cd ~/Projects/amplifier-usage-insights

# Install in development mode
pip install -e .

# Install dev dependencies
pip install -e ".[dev]"

# Initialize database
amplifier-insights init

# Scan existing sessions
amplifier-insights refresh

# Check status
amplifier-insights status
```

### Amplifier Bundle Setup

```bash
# Link bundle to Amplifier
ln -s ~/Projects/amplifier-usage-insights/bundles/usage-insights \
      ~/.amplifier/bundles/usage-insights

# Verify installation
amplifier bundles list
â†’ Should show "usage-insights"
```

### Usage

**In Terminal:**
```bash
# Get weekly summary
amplifier-insights show weekly

# See tool usage
amplifier-insights show tools

# Check growth
amplifier-insights show growth
```

**In Amplifier Session:**
```
You: "How am I doing this week?"
Assistant: [calls get_usage_insights tool]
â†’ Returns formatted insights with metrics, growth, tips
```

---

## What Comes After V0.5

### If Validation Succeeds

**Signals:**
- I check insights 3+ times per week
- I follow at least 1 tip suggestion
- I feel more aware of my AI usage patterns
- I want richer visualizations

**Then Build:**
- V1.0 with web dashboard (4-6 additional weeks)
- LLM tips generation
- Real-time file watcher
- Advanced analytics

### If Validation Fails

**Signals:**
- I check less than 1x per week
- Tips feel irrelevant
- No behavior change
- Insights don't feel valuable

**Then Pivot:**
- Maybe: Just a simple CLI tool (no conversational)
- Maybe: Focus on different metrics
- Maybe: Abandon (problem not real)

---

## Summary

**V0.5 is designed to answer ONE question:**

> **"Will conversational insights about my AI usage change my behavior?"**

**What we're building:**
- Session parser that reads Amplifier data
- Basic metrics (tool usage, duration, delegations)
- Simple growth tracking (this week vs last)
- Conversational tool for Amplifier
- Rule-based improvement tips

**What we're skipping:**
- Web dashboard
- LLM sophistication
- Real-time updates
- Advanced analytics

**Build time:** 3-6 hours  
**Validation time:** 2 weeks of personal use  
**Decision point:** Build V1.0 or pivot based on actual usage

**This is lean validation at its best - build the minimum to learn the maximum.**

---

## Related Documentation

- [VISION.md](../01-vision/VISION.md) - Why we're building this
- [PRINCIPLES.md](../01-vision/PRINCIPLES.md) - How we make decisions
- [Epic 01: Personal Insights](../02-requirements/epics/01-personal-insights.md) - Full V1 vision
- [V1-ARCHITECTURE.md](V1-ARCHITECTURE.md) - Complete architecture (what comes after V0.5)

---

## Change History

| Version | Date | Author | Changes |
|---------|------|--------|------------|
| v1.0 | 2026-02-03 | Chris Park (with AI) | Initial V0.5 validation build design |

---
