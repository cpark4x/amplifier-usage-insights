# V1 Technical Architecture: Personal Insights

**Owner:** Chris Park  
**Contributors:** Chris Park (with AI)

**Last Updated:** 2026-02-03

---

## Summary

This document defines the complete technical architecture for Amplifier Usage Insights V1 (Personal Insights). It covers system components, data models, technology choices, integration patterns, and implementation strategy for building a privacy-first analytics system that helps individuals master AI collaboration.

**Key Architectural Decisions:**
- **Local-first storage** (SQLite) - Privacy by design, no server required for V1
- **Incremental computation** - Process sessions as they complete, not batch
- **Conversational-first interface** - Amplifier tool is primary, dashboard is secondary
- **Simple metrics first** - Start with transparent, explainable calculations
- **Python + Vue.js** - Leverage Amplifier ecosystem + modern web tech

---

## Table of Contents

1. [System Architecture](#1-system-architecture)
2. [Data Models](#2-data-models)
3. [Technology Stack](#3-technology-stack)
4. [Module Structure](#4-module-structure)
5. [Integration Approach](#5-integration-approach)
6. [Deployment Strategy](#6-deployment-strategy)
7. [Development Phases](#7-development-phases)
8. [Testing Strategy](#8-testing-strategy)
9. [Critical Design Decisions](#9-critical-design-decisions)
10. [Future Considerations](#10-future-considerations)

---

## 1. System Architecture

### High-Level Component View

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Conversational Interface   â”‚      Web Dashboard (Vue.js)      â”‚
â”‚   (Amplifier Tool/Agent)     â”‚   (Static Site + API Backend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    Insights Engine   â”‚
                 â”‚   (Query Interface)  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        Analytics Core                â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â€¢ Metrics Calculator                â”‚
         â”‚  â€¢ Growth Analyzer                   â”‚
         â”‚  â€¢ Pattern Detector                  â”‚
         â”‚  â€¢ Tips Generator (LLM-based)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Computed Metrics DB    â”‚
         â”‚   (SQLite - Local)       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Session Data Ingestion  â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â€¢ Session Parser        â”‚
         â”‚  â€¢ Event Processor       â”‚
         â”‚  â€¢ Incremental Updater   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Amplifier Sessions     â”‚
         â”‚  ~/.amplifier/projects/  â”‚
         â”‚  â€¢ events.jsonl          â”‚
         â”‚  â€¢ transcript.jsonl      â”‚
         â”‚  â€¢ metadata.json         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. Session Ends
   â†“
2. File Watcher detects new/updated session files
   â†“
3. Session Parser reads events.jsonl, transcript.jsonl, metadata.json
   â†“
4. Event Processor extracts metrics (tools used, duration, turns, errors)
   â†“
5. Incremental Updater updates computed metrics (aggregations, trends)
   â†“
6. Metrics stored in local SQLite database
   â†“
7a. User asks "How am I doing?" â†’ Conversational Interface queries Insights Engine
7b. User opens dashboard â†’ Web app queries Insights Engine
   â†“
8. Insights Engine returns formatted results
   â†“
9. Tips Generator (LLM) creates actionable suggestions based on patterns
```

### Component Responsibilities

| Component | Responsibility | Input | Output |
|-----------|---------------|-------|--------|
| **Session Parser** | Parse Amplifier session files | events.jsonl, transcript.jsonl, metadata.json | Structured session data |
| **Event Processor** | Extract metrics from parsed data | Structured session data | Raw metrics |
| **Incremental Updater** | Update aggregations incrementally | Raw metrics | Updated computed metrics |
| **Metrics Calculator** | Compute derived metrics | Raw + computed metrics | Calculated insights |
| **Growth Analyzer** | Detect trends and changes | Time-series metrics | Growth indicators |
| **Pattern Detector** | Identify usage patterns | Session history | Pattern insights |
| **Tips Generator** | Create actionable suggestions | Patterns + metrics | Actionable tips |
| **Insights Engine** | Query interface for all insights | User queries | Formatted results |
| **Conversational Interface** | Natural language queries | User questions | Conversational responses |
| **Web Dashboard** | Visual exploration | Dashboard requests | Charts + tables |

---

## 2. Data Models

### Core Entities

#### **Session** (Raw Data - Parsed from Amplifier)

```python
@dataclass
class Session:
    """Represents a single Amplifier session."""
    
    # Identity
    session_id: str              # From metadata
    project_path: str            # Project directory
    
    # Timing
    started_at: datetime
    ended_at: datetime
    duration_seconds: int
    
    # Basic metrics
    turn_count: int
    model_used: str              # Primary model
    status: str                  # completed, abandoned, error
    
    # Derived from events.jsonl
    tools_used: list[str]        # ["bash", "read_file", "delegate"]
    tool_call_count: int
    tool_call_distribution: dict[str, int]  # {"bash": 12, "read_file": 8}
    
    file_paths_touched: list[str]
    error_count: int
    delegation_count: int
    delegated_agents: list[str]  # Which agents were used
    
    # Parsed from transcript
    user_message_count: int
    assistant_message_count: int
    total_tokens: Optional[int]  # If available
    
    # Privacy: We do NOT store actual content, prompts, or code
```

#### **SessionMetrics** (Computed - Stored in DB)

```python
@dataclass
class SessionMetrics:
    """Computed metrics for a single session."""
    
    session_id: str
    computed_at: datetime
    
    # Complexity indicators
    unique_tools_used: int
    tool_diversity_score: float  # Shannon entropy of tool usage
    files_touched: int
    repos_accessed: int
    
    # Effectiveness indicators
    time_per_turn: float         # duration / turn_count
    tool_calls_per_turn: float
    error_rate: float            # errors / total_actions
    delegation_ratio: float      # delegations / total_actions
    
    # Task classification (high-level, no sensitive content)
    primary_task_category: str   # "coding", "debugging", "research", etc.
    complexity_estimate: str     # "low", "medium", "high"
    
    # Session outcome
    completion_status: str       # "completed", "abandoned", "error_terminated"
```

#### **UserMetrics** (Aggregated - Time Windows)

```python
@dataclass
class UserMetrics:
    """Aggregated metrics for a user over a time period."""
    
    user_id: str                 # Default: "local" (for V1)
    time_period: str             # "week_2026_05", "month_2026_02"
    period_start: datetime
    period_end: datetime
    
    # Volume metrics
    session_count: int
    total_duration_seconds: int
    total_turns: int
    total_tool_calls: int
    
    # Tool sophistication
    unique_tools_used: int
    avg_tool_diversity: float
    tool_usage_distribution: dict[str, int]  # {"bash": 45, "grep": 12}
    most_used_tools: list[str]   # Top 5
    
    # Effectiveness
    avg_time_per_turn: float
    avg_error_rate: float
    avg_delegation_ratio: float
    avg_session_duration: float
    
    # Growth indicators (compare to previous period)
    sessions_vs_prev: float      # +15% or -10%
    tool_diversity_vs_prev: float
    effectiveness_vs_prev: float
    error_rate_vs_prev: float
    
    # Calculated at query time, not stored
    growth_direction: str        # "improving", "declining", "stable"
```

#### **GrowthMetric** (Trends Over Time)

```python
@dataclass
class GrowthMetric:
    """Trend analysis for a specific metric over time."""
    
    user_id: str
    metric_name: str             # "tool_diversity", "error_rate", etc.
    
    # Time series data
    time_series: list[tuple[datetime, float]]
    
    # Trend analysis
    trend_direction: str         # "improving", "declining", "stable"
    trend_strength: float        # -1.0 to +1.0 (correlation coefficient)
    recent_change: float         # % change in last period vs previous
    
    # Statistical
    mean: float
    std_dev: float
    min_value: float
    max_value: float
```

#### **ActionableTip** (Generated Insights)

```python
@dataclass
class ActionableTip:
    """An actionable suggestion based on usage patterns."""
    
    user_id: str
    generated_at: datetime
    category: str                # "tool_usage", "delegation", "error_handling"
    priority: str                # "high", "medium", "low"
    
    # The tip
    observation: str             # "You use bash 3x more than specialized tools"
    recommendation: str          # "Try using grep for file searches instead of bash"
    expected_benefit: str        # "30% faster file operations"
    
    # Context
    based_on_sessions: list[str] # Session IDs that triggered this tip
    metric_values: dict[str, float]  # Relevant metrics
    
    # Lifecycle
    shown_to_user: bool
    dismissed: bool
    marked_helpful: Optional[bool]
```

---

## 3. Technology Stack

### Core Application (Python)

```toml
[project]
name = "amplifier-usage-insights"
version = "0.1.0"
requires-python = ">=3.11"

dependencies = [
    # Data Processing
    "polars>=0.20.0",           # Fast dataframe operations
    "python-dateutil>=2.8.0",   # Date parsing
    
    # Amplifier Integration
    "amplifier-core",           # For session file formats
    
    # LLM for Tips Generation
    "litellm>=1.0.0",           # Universal LLM interface
    
    # Web API
    "fastapi>=0.109.0",         # API framework
    "uvicorn[standard]>=0.27.0", # ASGI server
    "pydantic>=2.5.0",          # Data validation
    
    # CLI
    "typer>=0.9.0",             # CLI framework
    "rich>=13.0.0",             # Terminal output
    
    # File watching
    "watchdog>=4.0.0",          # Monitor session directory
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.2.0",
    "pyright>=1.1.0",
]
```

**Key Library Choices:**

| Library | Purpose | Why This Choice |
|---------|---------|-----------------|
| **Polars** | Data processing | 10-100x faster than pandas, better for time-series |
| **SQLite** | Storage | Built-in, no setup, privacy-first, scales to millions of rows |
| **LiteLLM** | LLM access | Works with any provider, uses user's Amplifier config |
| **FastAPI** | API backend | Fast, modern, automatic OpenAPI docs, async support |
| **Typer** | CLI | Beautiful CLI with auto-completion, minimal boilerplate |
| **Watchdog** | File monitoring | Cross-platform file system events |

### Web Dashboard (Vue.js)

```json
{
  "name": "amplifier-usage-insights-web",
  "version": "0.1.0",
  "dependencies": {
    "vue": "^3.4.0",
    "vite": "^5.0.0",
    "pinia": "^2.1.0",
    "vue-router": "^4.2.0",
    "chart.js": "^4.4.0",
    "vue-chartjs": "^5.3.0",
    "tailwindcss": "^3.4.0",
    "axios": "^1.6.0",
    "date-fns": "^3.0.0"
  },
  "devDependencies": {
    "@vitejs/plugin-vue": "^5.0.0",
    "autoprefixer": "^10.4.0",
    "postcss": "^8.4.0"
  }
}
```

**Key Library Choices:**

| Library | Purpose | Why This Choice |
|---------|---------|-----------------|
| **Vue 3** | Framework | Reactive, lightweight, easier than React for solo dev |
| **Vite** | Build tool | Lightning-fast dev server and builds |
| **Pinia** | State management | Official Vue store, simpler than Vuex |
| **Chart.js** | Visualization | Simple, beautiful charts without D3 complexity |
| **Tailwind CSS** | Styling | Utility-first, rapid UI development |

### Storage Schema (SQLite)

```sql
-- ~/.amplifier-usage-insights/metrics.db

-- Raw sessions (parsed from Amplifier)
CREATE TABLE sessions (
    session_id TEXT PRIMARY KEY,
    project_path TEXT NOT NULL,
    started_at TIMESTAMP NOT NULL,
    ended_at TIMESTAMP NOT NULL,
    duration_seconds INTEGER NOT NULL,
    turn_count INTEGER NOT NULL,
    model_used TEXT,
    status TEXT NOT NULL,
    tool_call_count INTEGER NOT NULL,
    error_count INTEGER NOT NULL,
    delegation_count INTEGER NOT NULL,
    user_message_count INTEGER NOT NULL,
    assistant_message_count INTEGER NOT NULL,
    total_tokens INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Session metrics (computed)
CREATE TABLE session_metrics (
    session_id TEXT PRIMARY KEY,
    computed_at TIMESTAMP NOT NULL,
    unique_tools_used INTEGER NOT NULL,
    tool_diversity_score REAL NOT NULL,
    files_touched INTEGER NOT NULL,
    repos_accessed INTEGER NOT NULL,
    time_per_turn REAL NOT NULL,
    tool_calls_per_turn REAL NOT NULL,
    error_rate REAL NOT NULL,
    delegation_ratio REAL NOT NULL,
    primary_task_category TEXT,
    complexity_estimate TEXT,
    completion_status TEXT NOT NULL,
    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);

-- Tool usage per session
CREATE TABLE session_tools (
    session_id TEXT NOT NULL,
    tool_name TEXT NOT NULL,
    call_count INTEGER NOT NULL,
    PRIMARY KEY (session_id, tool_name),
    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
);

-- Aggregated metrics by time period
CREATE TABLE user_metrics (
    user_id TEXT NOT NULL,
    time_period TEXT NOT NULL,
    period_start TIMESTAMP NOT NULL,
    period_end TIMESTAMP NOT NULL,
    session_count INTEGER NOT NULL,
    total_duration_seconds INTEGER NOT NULL,
    total_turns INTEGER NOT NULL,
    total_tool_calls INTEGER NOT NULL,
    unique_tools_used INTEGER NOT NULL,
    avg_tool_diversity REAL NOT NULL,
    avg_time_per_turn REAL NOT NULL,
    avg_error_rate REAL NOT NULL,
    avg_delegation_ratio REAL NOT NULL,
    sessions_vs_prev REAL,
    tool_diversity_vs_prev REAL,
    effectiveness_vs_prev REAL,
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (user_id, time_period)
);

-- Tool usage by time period
CREATE TABLE period_tools (
    user_id TEXT NOT NULL,
    time_period TEXT NOT NULL,
    tool_name TEXT NOT NULL,
    call_count INTEGER NOT NULL,
    PRIMARY KEY (user_id, time_period, tool_name)
);

-- Actionable tips
CREATE TABLE actionable_tips (
    tip_id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    generated_at TIMESTAMP NOT NULL,
    category TEXT NOT NULL,
    priority TEXT NOT NULL,
    observation TEXT NOT NULL,
    recommendation TEXT NOT NULL,
    expected_benefit TEXT NOT NULL,
    shown_to_user BOOLEAN DEFAULT FALSE,
    dismissed BOOLEAN DEFAULT FALSE,
    marked_helpful BOOLEAN
);

-- Indexes for common queries
CREATE INDEX idx_sessions_started_at ON sessions(started_at);
CREATE INDEX idx_sessions_project_path ON sessions(project_path);
CREATE INDEX idx_user_metrics_period ON user_metrics(user_id, period_start);
CREATE INDEX idx_tips_user_shown ON actionable_tips(user_id, shown_to_user);
```

---

## 4. Module Structure

### Directory Layout

```
amplifier-usage-insights/
â”œâ”€â”€ pyproject.toml                 # Python package config
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/                          # Existing vision/requirements
â”‚   â”œâ”€â”€ 01-vision/
â”‚   â”œâ”€â”€ 02-requirements/
â”‚   â””â”€â”€ 03-technical-architecture/
â”‚       â””â”€â”€ V1-ARCHITECTURE.md     # This document
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ amplifier_usage_insights/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __main__.py            # Entry point for CLI
â”‚       â”‚
â”‚       â”œâ”€â”€ core/                  # Analytics Core
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ models.py          # Data models (Session, Metrics, etc.)
â”‚       â”‚   â”œâ”€â”€ metrics.py         # Metrics calculation logic
â”‚       â”‚   â”œâ”€â”€ growth.py          # Growth analysis and trends
â”‚       â”‚   â”œâ”€â”€ patterns.py        # Pattern detection
â”‚       â”‚   â””â”€â”€ tips.py            # Tips generation (LLM-based)
â”‚       â”‚
â”‚       â”œâ”€â”€ ingestion/             # Session Data Ingestion
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ parser.py          # Parse Amplifier session files
â”‚       â”‚   â”œâ”€â”€ processor.py       # Extract metrics from parsed data
â”‚       â”‚   â”œâ”€â”€ watcher.py         # File watcher for new sessions
â”‚       â”‚   â””â”€â”€ updater.py         # Incremental metric updates
â”‚       â”‚
â”‚       â”œâ”€â”€ storage/               # Database Layer
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ schema.sql         # SQLite schema
â”‚       â”‚   â”œâ”€â”€ db.py              # Database operations
â”‚       â”‚   â””â”€â”€ migrations/        # Schema migrations
â”‚       â”‚       â””â”€â”€ 001_initial.sql
â”‚       â”‚
â”‚       â”œâ”€â”€ insights/              # Insights Engine
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ engine.py          # Query interface
â”‚       â”‚   â”œâ”€â”€ formatters.py      # Format results for different interfaces
â”‚       â”‚   â””â”€â”€ queries.py         # Common query patterns
â”‚       â”‚
â”‚       â”œâ”€â”€ interfaces/            # User Interfaces
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ api.py             # FastAPI backend
â”‚       â”‚   â”œâ”€â”€ conversational.py  # Conversational tool/agent
â”‚       â”‚   â””â”€â”€ cli.py             # CLI commands (typer)
â”‚       â”‚
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ config.py          # Configuration management
â”‚           â””â”€â”€ logging.py         # Logging setup
â”‚
â”œâ”€â”€ bundles/                       # Amplifier Bundle
â”‚   â””â”€â”€ usage-insights/
â”‚       â”œâ”€â”€ bundle.yaml            # Bundle definition
â”‚       â”œâ”€â”€ tools.yaml             # Tool definitions
â”‚       â””â”€â”€ contexts/
â”‚           â””â”€â”€ instructions.md    # How to use the insights tool
â”‚
â”œâ”€â”€ web/                           # Vue.js Dashboard
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.js
â”‚   â”‚   â”œâ”€â”€ App.vue
â”‚   â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”‚   â”œâ”€â”€ stores/                # Pinia stores
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.js
â”‚   â”‚   â”‚   â””â”€â”€ sessions.js
â”‚   â”‚   â”œâ”€â”€ components/            # Reusable components
â”‚   â”‚   â”‚   â”œâ”€â”€ MetricCard.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ TrendChart.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ TipsList.vue
â”‚   â”‚   â”‚   â””â”€â”€ SessionTable.vue
â”‚   â”‚   â”œâ”€â”€ views/                 # Page views
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ GrowthView.vue
â”‚   â”‚   â”‚   â”œâ”€â”€ SessionsView.vue
â”‚   â”‚   â”‚   â””â”€â”€ TipsView.vue
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ api.js             # API client
â”‚   â””â”€â”€ public/
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ test_parser.py
    â”‚   â”œâ”€â”€ test_metrics.py
    â”‚   â””â”€â”€ test_growth.py
    â”œâ”€â”€ integration/
    â”‚   â”œâ”€â”€ test_ingestion_pipeline.py
    â”‚   â””â”€â”€ test_insights_engine.py
    â””â”€â”€ fixtures/
        â””â”€â”€ sample_sessions/       # Mock Amplifier sessions
```

---

## 5. Integration Approach

### Conversational Interface (Primary)

**Amplifier Tool** - Natural language queries within any Amplifier session

```yaml
# bundles/usage-insights/tools.yaml
tools:
  - name: get_personal_insights
    description: |
      Get insights about your AI collaboration effectiveness.
      
      Ask questions like:
      - "How am I doing this week?"
      - "Show my growth over the last month"
      - "What tools do I use most?"
      - "Give me tips to improve"
      - "Compare this week to last week"
    
    parameters:
      query:
        type: string
        description: Natural language question about usage patterns
        required: true
      
      time_range:
        type: string
        enum: ["week", "month", "90days", "all"]
        default: "week"
        description: Time period for analysis
    
    returns:
      type: object
      properties:
        summary:
          type: string
          description: High-level answer to the query
        
        metrics:
          type: object
          description: Key metrics relevant to the query
        
        tips:
          type: array
          items: {type: string}
          description: Actionable suggestions
        
        growth_indicators:
          type: object
          description: Improvement trends
        
        details_url:
          type: string
          description: Link to dashboard for deep dive
```

**Usage Example:**

```
User: "How's my AI usage this week?"

Assistant (via tool): {
  "summary": "You're showing strong growth! ðŸš€",
  "metrics": {
    "sessions": {
      "count": 12,
      "vs_last_week": "+50%"
    },
    "tool_sophistication": {
      "unique_tools": 8,
      "diversity_score": 2.1,
      "vs_last_week": "+15%"
    },
    "effectiveness": {
      "avg_time_per_turn": "45s",
      "vs_last_week": "-10% (faster)"
    }
  },
  "tips": [
    {
      "priority": "high",
      "observation": "You use bash 3x more than specialized tools",
      "recommendation": "Try using grep for file searches instead of 'bash cat file | bash grep'",
      "expected_benefit": "30% faster file operations"
    }
  ],
  "growth_indicators": {
    "overall_trend": "improving",
    "weeks_of_improvement": 3,
    "strongest_area": "delegation patterns"
  },
  "details_url": "http://localhost:8080/dashboard"
}

Assistant responds:
"You're showing strong growth this week! ðŸš€

**This Week vs Last Week:**
- 12 sessions (+50% from 8 last week)
- Tool sophistication up 15% (using 8 different tools effectively)
- 10% faster per turn (45s avg vs 50s last week)

**Growth Area: Tool Usage**
I notice you're using bash 3x more than specialized tools. Try using `grep` for file searches instead of `bash cat | grep` chains - you could see 30% faster file operations.

Want to see your full dashboard with charts and details? [Open Dashboard](http://localhost:8080/dashboard)"
```

**Implementation:**

```python
# src/amplifier_usage_insights/interfaces/conversational.py

from amplifier_usage_insights.insights.engine import InsightsEngine
from amplifier_usage_insights.insights.formatters import ConversationalFormatter

async def get_personal_insights(query: str, time_range: str = "week") -> dict:
    """Tool implementation for conversational interface."""
    
    engine = InsightsEngine()
    formatter = ConversationalFormatter()
    
    # Parse natural language query into structured query
    structured_query = engine.parse_query(query, time_range)
    
    # Execute query
    results = await engine.execute(structured_query)
    
    # Format for conversational response
    response = formatter.format(results)
    
    return response
```

---

### Web Dashboard (Secondary)

**Static Vue.js SPA** - Visual exploration and deep analysis

**Key Views:**

1. **Dashboard (Home)**
   - Summary cards: Sessions this week, growth %, top tools
   - Growth trend chart (last 30 days)
   - Recent actionable tips
   - Quick stats: Error rate, delegation ratio, avg session time

2. **Growth View**
   - Time-series charts for key metrics
   - Week-over-week comparison tables
   - Growth indicators by category
   - Historical performance

3. **Sessions View**
   - Table of all sessions with filters (date, project, duration)
   - Session details drill-down
   - Tool usage per session
   - Search and export

4. **Tips View**
   - All actionable tips (prioritized)
   - Filter by category
   - Mark as helpful/dismiss
   - Track which tips were acted on

**API Endpoints:**

```python
# src/amplifier_usage_insights/interfaces/api.py

from fastapi import FastAPI, Query
from datetime import datetime, timedelta

app = FastAPI(title="Amplifier Usage Insights API")

@app.get("/api/v1/metrics/summary")
async def get_summary(
    time_range: str = Query("week", enum=["week", "month", "90days", "all"])
):
    """Get summary metrics for dashboard."""
    pass

@app.get("/api/v1/metrics/growth")
async def get_growth(
    metric: str = Query(...),
    start_date: datetime = None,
    end_date: datetime = None
):
    """Get time-series data for a specific metric."""
    pass

@app.get("/api/v1/sessions")
async def list_sessions(
    start_date: datetime = None,
    end_date: datetime = None,
    project_path: str = None,
    limit: int = 50,
    offset: int = 0
):
    """List sessions with pagination and filters."""
    pass

@app.get("/api/v1/sessions/{session_id}")
async def get_session(session_id: str):
    """Get detailed metrics for a single session."""
    pass

@app.get("/api/v1/tips")
async def get_tips(
    category: str = None,
    priority: str = None,
    shown: bool = None
):
    """Get actionable tips."""
    pass

@app.post("/api/v1/tips/{tip_id}/feedback")
async def tip_feedback(tip_id: str, helpful: bool):
    """Mark a tip as helpful or not."""
    pass
```

---

## 6. Deployment Strategy

### V1.0 Deployment (Local-First)

**Installation:**

```bash
# Install Python package
pip install amplifier-usage-insights

# Initialize (creates DB, starts file watcher)
amplifier-insights init

# Start background service (watches for new sessions)
amplifier-insights start

# Open dashboard
amplifier-insights dashboard
```

**Components:**

1. **Background Service** (watchdog)
   - Monitors `~/.amplifier/projects/` for new/updated sessions
   - Processes sessions incrementally
   - Updates metrics in real-time
   - Runs as systemd service (Linux) or launchd (macOS)

2. **API Server** (FastAPI)
   - Runs on `localhost:8080` (configurable)
   - Serves dashboard + API endpoints
   - Auto-starts with background service

3. **Web Dashboard** (Vue.js)
   - Static files served by FastAPI
   - Communicates with API via localhost

**File Locations:**

```
~/.amplifier-usage-insights/
â”œâ”€â”€ metrics.db              # SQLite database
â”œâ”€â”€ config.yaml             # User configuration
â””â”€â”€ logs/
    â””â”€â”€ insights.log        # Service logs
```

### Configuration

```yaml
# ~/.amplifier-usage-insights/config.yaml

# Data sources
amplifier:
  session_dirs:
    - ~/.amplifier/projects/
    - ~/custom/sessions/

# Processing
processing:
  watch_enabled: true
  batch_size: 10
  incremental_update: true

# Privacy
privacy:
  store_content: false       # Never store prompts/code
  redact_file_paths: true    # Store only filenames, not full paths
  retention_days: 365        # Delete data older than 1 year

# API
api:
  host: localhost
  port: 8080
  cors_enabled: false        # Disable CORS for local-only

# Tips generation
tips:
  llm_provider: litellm      # Use user's Amplifier LLM config
  generation_frequency: daily
  max_tips_per_day: 5
```

### Security Considerations

**V1 is local-only:**
- No data leaves the machine
- API bound to localhost only
- No authentication needed (single user)
- SQLite database with user-only permissions

---

## 7. Development Phases

### Phase 0: Foundation (Week 1-2)

**Goal:** Set up project structure, basic ingestion pipeline

**Deliverables:**
- âœ… Project structure created
- âœ… Python package setup (pyproject.toml)
- âœ… Database schema (SQLite)
- âœ… Session parser (reads events.jsonl, transcript.jsonl, metadata.json)
- âœ… Basic CLI (`amplifier-insights init`, `amplifier-insights parse <session>`)

**Success Criteria:**
- Can parse an Amplifier session and extract basic metrics
- Metrics stored in SQLite
- No errors on sample sessions

---

### Phase 1: Core Analytics (Week 3-4)

**Goal:** Implement metrics calculation and storage

**Deliverables:**
- âœ… Metrics calculator (session-level metrics)
- âœ… Aggregation logic (user metrics by time period)
- âœ… Incremental updater (process new sessions)
- âœ… File watcher (detect new sessions automatically)
- âœ… CLI commands: `amplifier-insights status`, `amplifier-insights metrics`

**Success Criteria:**
- File watcher detects new sessions and processes them
- Metrics update incrementally (no full recompute)
- Can query metrics via CLI
- Performance: Process 100 sessions in <10 seconds

---

### Phase 2: Conversational Interface (Week 5-6)

**Goal:** Enable natural language queries within Amplifier

**Deliverables:**
- âœ… Insights Engine (query parser + executor)
- âœ… Conversational formatter (human-readable responses)
- âœ… Amplifier bundle (`usage-insights`)
- âœ… Tool definition (`get_personal_insights`)
- âœ… Growth analyzer (trend detection)

**Success Criteria:**
- Can ask "How am I doing?" in Amplifier and get meaningful response
- Growth comparisons work (this week vs last week)
- Response time <2 seconds for typical queries
- User validation: 3+ users try it and find it valuable

---

### Phase 3: Tips Generation (Week 7)

**Goal:** Generate actionable improvement suggestions

**Deliverables:**
- âœ… Pattern detector (identify usage patterns)
- âœ… Tips generator (LLM-based suggestions)
- âœ… Tip storage and lifecycle (shown, dismissed, helpful)
- âœ… Integration with conversational interface

**Success Criteria:**
- Generates 3-5 relevant tips per week
- Tips are actionable (not generic advice)
- Users can provide feedback (helpful/not helpful)
- 60%+ of tips marked helpful by users

---

### Phase 4: Web Dashboard (Week 8-10)

**Goal:** Visual exploration and deep analysis

**Deliverables:**
- âœ… FastAPI backend (API endpoints)
- âœ… Vue.js frontend (4 main views)
- âœ… Chart components (Chart.js integration)
- âœ… Responsive design (works on laptop + tablet)
- âœ… CLI command: `amplifier-insights dashboard`

**Success Criteria:**
- Dashboard loads in <2 seconds
- All charts render correctly
- Can drill down into individual sessions
- Users prefer dashboard for deep exploration

---

### Phase 5: Polish & Launch (Week 11-12)

**Goal:** Production-ready V1.0

**Deliverables:**
- âœ… Comprehensive tests (80%+ coverage)
- âœ… Documentation (installation, usage, troubleshooting)
- âœ… Error handling and logging
- âœ… Performance optimization
- âœ… Package published (PyPI)
- âœ… Launch blog post + demo video

**Success Criteria:**
- Passes all tests
- No critical bugs in alpha testing
- Installation works on macOS + Linux
- 10+ alpha users providing feedback
- Positive reception from early users

---

## 8. Testing Strategy

### Unit Tests

**Coverage Target:** 80%+

**Key Areas:**
- Session parser (various event types, edge cases)
- Metrics calculator (correct math, edge cases like 0 sessions)
- Growth analyzer (trend detection accuracy)
- Pattern detector (pattern identification)
- Formatters (output correctness)

**Example:**

```python
# tests/unit/test_metrics.py

def test_tool_diversity_score():
    """Test Shannon entropy calculation for tool usage."""
    session = Session(
        tool_call_distribution={"bash": 10, "read_file": 5, "grep": 5}
    )
    metrics = calculate_session_metrics(session)
    
    # Shannon entropy for this distribution
    expected_diversity = 1.055  # Calculated manually
    assert abs(metrics.tool_diversity_score - expected_diversity) < 0.01

def test_growth_comparison_with_no_previous_period():
    """Handle case where there's no previous period to compare."""
    user_metrics = UserMetrics(...)
    growth = calculate_growth(user_metrics, previous=None)
    
    assert growth.sessions_vs_prev is None
    assert growth.trend_direction == "stable"
```

---

### Integration Tests

**Scenarios:**
1. **End-to-End Ingestion**
   - Place sample session in watch directory
   - Verify it's parsed, processed, and stored
   - Query metrics and verify correctness

2. **Incremental Update**
   - Process 10 sessions
   - Add 1 new session
   - Verify only new session is processed

3. **API Endpoints**
   - Test all REST endpoints
   - Verify response formats
   - Test error cases (invalid session ID, etc.)

---

### Fixture Data

**Mock Amplifier Sessions:**

Create realistic fixtures in `tests/fixtures/sample_sessions/`:

- `simple_session/` - Basic session with a few tool calls
- `complex_session/` - Large session with many tools and delegations
- `error_session/` - Session with errors and retries
- `delegation_heavy/` - Multiple agent delegations
- `long_session/` - High turn count, long duration

---

### Performance Tests

**Benchmarks:**

- Parse 100 sessions: <10 seconds
- Calculate metrics for 1000 sessions: <30 seconds
- API response time (summary): <500ms
- Dashboard load time: <2 seconds
- File watcher latency: <5 seconds from session end to metrics update

---

## 9. Critical Design Decisions

### Decision 1: Local SQLite vs. Remote Database

**Choice:** Local SQLite

**Reasoning:**
- âœ… Privacy by design (data never leaves machine)
- âœ… No server setup required
- âœ… Scales to millions of sessions on laptop
- âœ… Simple deployment (pip install + run)
- âœ… Aligns with V1 scope (personal insights)

**Trade-offs:**
- âŒ Can't support team features without data sharing mechanism
- âŒ No real-time sync across devices

**Future:** V2 will add optional cloud sync for team features.

---

### Decision 2: Polars vs. Pandas

**Choice:** Polars

**Reasoning:**
- âœ… 10-100x faster for time-series operations
- âœ… Better memory efficiency
- âœ… Modern API (less legacy baggage)
- âœ… Better type safety

**Trade-offs:**
- âŒ Smaller ecosystem than Pandas
- âŒ Less familiar to many developers

**Mitigation:** Use Polars for internal processing, expose standard Python types in API.

---

### Decision 3: LLM Tips Generation vs. Rule-Based

**Choice:** LLM-based with rule-based fallback

**Reasoning:**
- âœ… More nuanced, contextual suggestions
- âœ… Can explain "why" not just "what"
- âœ… Adapts to usage patterns naturally
- âœ… Uses user's existing Amplifier LLM config

**Trade-offs:**
- âŒ Requires LLM access (costs money)
- âŒ Slower than rule-based (1-2s latency)
- âŒ Less predictable output

**Mitigation:** Cache tips, generate daily not on-demand. Provide rule-based tips if LLM unavailable.

---

### Decision 4: Conversational-First vs. Dashboard-First

**Choice:** Conversational-first

**Reasoning:**
- âœ… Aligns with principle "Conversational First, Dashboard Second"
- âœ… Lower barrier to value (no context switch)
- âœ… Faster to build and validate
- âœ… Natural for users already in Amplifier

**Trade-offs:**
- âŒ Less rich visualization initially
- âŒ Dashboard features delayed

**Sequencing:** Phase 2 = Conversational, Phase 4 = Dashboard (validate engagement first).

---

### Decision 5: Real-Time vs. Batch Processing

**Choice:** Real-time (incremental) with file watcher

**Reasoning:**
- âœ… Aligns with principle "Incremental Computation"
- âœ… Insights available immediately after session
- âœ… Better user experience (no waiting)
- âœ… Scales better (no expensive batch jobs)

**Trade-offs:**
- âŒ More complex (file watching, event-driven)
- âŒ Background service required

**Mitigation:** Make background service optional - CLI can do one-off batch processing.

---

## 10. Future Considerations (V2+)

### V2: Team Insights

**Architectural Changes Needed:**

1. **Multi-User Support**
   - User ID field (currently hardcoded to "local")
   - Authentication system
   - Authorization (who can see team metrics)

2. **Data Aggregation Service**
   - Collect metrics from multiple users
   - Compute team-level aggregations
   - Privacy: aggregated metrics only, no raw session data

3. **Team Database**
   - Centralized storage for team metrics
   - Could be self-hosted or cloud service
   - Encryption at rest

**Current Architecture Supports:**
- âœ… User ID field exists in all models (currently "local")
- âœ… Metrics are already aggregated (easy to combine across users)
- âœ… Tool usage patterns are comparable (standardized metrics)

**Gaps to Address:**
- Need: Secure data sharing mechanism
- Need: Team roster management
- Need: Public team dashboard UI

---

### V3: Manager Insights

**Architectural Changes Needed:**

1. **Coaching Context**
   - Manager-specific views (not just aggregations)
   - Coaching opportunity detection (alerts)
   - Historical coaching log

2. **Comparative Analytics**
   - High performer pattern identification
   - Struggling user early detection
   - Team capability benchmarking

**Current Architecture Supports:**
- âœ… All metrics needed for coaching already computed
- âœ… Growth trends track improvement/decline
- âœ… Pattern detection identifies behaviors

**Gaps to Address:**
- Need: Alert system for coaching opportunities
- Need: Manager-specific UI views
- Need: Integration with performance management systems

---

### Extensibility: Other AI Tools

**Current Architecture:**

```python
# Session parser is Amplifier-specific
class AmplifierSessionParser:
    def parse(self, session_dir: Path) -> Session:
        # Parse events.jsonl, transcript.jsonl
        pass

# But data model is generic
@dataclass
class Session:
    """Generic AI tool session - not Amplifier-specific."""
    session_id: str
    started_at: datetime
    tools_used: list[str]  # Works for any AI tool
    # ...
```

**To Add Another Tool (e.g., Claude Desktop):**

1. Create `ClaudeSessionParser` (implement common interface)
2. Register parser in config
3. All downstream logic works (metrics, growth, tips)

**Design Principle Applied:** "Extensible to Other AI Tools" (Principle #4)

---

## 11. Open Questions & Risks

### Open Questions

1. **Metric Validation**
   - **Q:** Do the metrics we compute actually correlate with user skill?
   - **Validation:** Compare metrics to user self-assessment + peer reviews
   - **Timeline:** Phase 5 (alpha testing)

2. **Privacy Comfort Level**
   - **Q:** Are users comfortable with local analytics on their work?
   - **Validation:** User interviews during alpha
   - **Mitigation:** Transparent about what's tracked, easy opt-out

3. **Engagement Cadence**
   - **Q:** How often will users check insights?
   - **Hypothesis:** Weekly for conversational, monthly for dashboard
   - **Validation:** Measure actual usage patterns in alpha

4. **Tips Quality**
   - **Q:** Are LLM-generated tips actually helpful?
   - **Validation:** Tip feedback system (helpful/not helpful)
   - **Mitigation:** Curate tip templates, use rule-based fallback

---

### Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| **Amplifier session format changes** | H | M | Version format parsing, graceful degradation |
| **Low engagement after novelty** | H | M | Conversational interface reduces friction, focus on valuable metrics |
| **Metrics don't drive improvement** | H | M | Iterate on metrics based on user feedback, focus on actionable tips |
| **Performance issues with large session histories** | M | M | Incremental computation, database indexes, pagination |
| **Users don't trust the metrics** | H | L | Transparent calculations, show raw data, explainable metrics |
| **Privacy concerns block adoption** | M | L | Local-only storage, no cloud required, clear communication |

---

## 12. Summary

### What We're Building

**V1 Personal Insights** provides individual contributors with comprehensive analytics on their AI collaboration effectiveness through:

1. **Conversational Interface** - Natural language queries within Amplifier sessions
2. **Web Dashboard** - Visual exploration and deep analysis
3. **Growth Tracking** - Week-over-week improvement indicators
4. **Actionable Tips** - LLM-generated suggestions based on patterns

### Key Architectural Principles

- **Privacy by Design** - Local SQLite storage, no data leaves machine
- **Incremental Computation** - Real-time updates as sessions complete
- **Conversational First** - Primary interface is natural language, dashboard is secondary
- **Simple Metrics First** - Transparent, explainable calculations
- **Extensible** - Designed to support team features (V2) and other AI tools (future)

### Implementation Timeline

- **Weeks 1-2:** Foundation (project setup, session parsing)
- **Weeks 3-4:** Core Analytics (metrics calculation, file watching)
- **Weeks 5-6:** Conversational Interface (Amplifier integration)
- **Week 7:** Tips Generation (LLM-based suggestions)
- **Weeks 8-10:** Web Dashboard (Vue.js frontend)
- **Weeks 11-12:** Polish & Launch (testing, docs, alpha release)

**Total: 12 weeks to production-ready V1.0**

### Success Criteria

- âœ… 50%+ weekly active usage (users check insights weekly)
- âœ… 60%+ find value (rate insights as valuable)
- âœ… 40%+ behavior change (users change how they work)
- âœ… 50%+ retention at 60 days (still using 2 months later)

### Next Steps

1. **Create project structure** (directories, pyproject.toml)
2. **Implement session parser** (read Amplifier session files)
3. **Set up SQLite database** (schema + migrations)
4. **Build metrics calculator** (session-level metrics)
5. **Test with real sessions** (validate on your own Amplifier sessions)

---

## Related Documentation

- [VISION.md](../01-vision/VISION.md) - Strategic vision and positioning
- [PRINCIPLES.md](../01-vision/PRINCIPLES.md) - Implementation philosophy
- [SUCCESS-METRICS.md](../01-vision/SUCCESS-METRICS.md) - How we measure success
- [Epic 01: Personal Insights](../02-requirements/epics/01-personal-insights.md) - Feature requirements

---

## Change History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| v1.0 | 2026-02-03 | Chris Park (with AI) | Initial technical architecture for V1 |

---