# Amplifier Usage Insights: Success Metrics

**How we measure if Amplifier Usage Insights is achieving its vision of building world-class AI-first teams**

**Owner:** Chris Park  
**Contributors:** Chris Park

**Last Updated:** 2026-02-03

---

## Summary

Success for Amplifier Usage Insights is measured by whether users become measurably better at AI collaboration. Each phase has distinct validation criteria: V1 proves individuals find value in understanding their AI usage patterns; V2 proves teams learn from peer transparency; V3 proves managers can build world-class AI-first teams through data-driven coaching. Our primary indicator is **observable improvement in AI collaboration effectiveness**—the ultimate proof that measurement drives growth.

---

## Table of Contents

1. [Primary Success Indicator](#primary-success-indicator)
2. [V1 Metrics](#v1-metrics-personal-insights)
3. [V2 Metrics](#v2-metrics-team-insights)
4. [V3 Metrics](#v3-metrics-manager-insights)
5. [User Satisfaction Signals](#user-satisfaction-signals)
6. [What We Don't Measure](#what-we-dont-measure)
7. [Success Criteria by Phase](#success-criteria-by-phase)

---

## Primary Success Indicator

**"Are users measurably better at AI collaboration after using Amplifier Usage Insights?"**

This is the north star. All other metrics support this question. If users aren't improving their AI collaboration skills, the product has failed regardless of adoption, engagement, or satisfaction metrics.

Evidence of improvement:
- Users attempt more complex tasks over time
- Tool usage patterns become more sophisticated
- Time-to-outcome decreases for similar tasks
- Error rates decrease as patterns improve
- Users report feeling more confident and effective

---

## V1 Metrics: Personal Insights

**Phase Goal:** Validate that individuals find value in understanding their AI usage patterns and that insights drive behavior change.

### Leading Indicators (Predict Success)

**Engagement:**
- **Daily Active Users (DAU)** - 50%+ of installed users check insights at least weekly
- **Conversational queries** - 3+ natural language questions per user per week ("How am I doing?", "Show my growth")
- **Dashboard visits** - 2+ visits per user per week for deeper exploration
- **Return rate** - 70%+ of users return within 7 days of first use

**Feature Adoption:**
- **Growth tracking viewed** - 60%+ users check week-over-week improvement
- **Tips engagement** - 40%+ users click through to actionable suggestions
- **Historical exploration** - 30%+ users explore past session patterns

### Lagging Indicators (Measure Success)

**Behavior Change:**
- **Self-reported improvement** - 60%+ users say they've changed how they work with AI based on insights
- **Tool sophistication increase** - 50%+ users show measurable increase in tool variety/complexity over 30 days
- **Session effectiveness** - 40%+ users show improving impact-per-session metrics over time

**Value Perception:**
- **Perceived usefulness** - 70%+ users rate insights as "valuable" or "very valuable"
- **Would recommend** - 60%+ would recommend to teammates
- **Continued use** - 50%+ still using after 60 days (retention)

### Qualitative Signals

**User Quotes We Listen For:**
- "I didn't realize I was using bash so much—switching to specialized tools helped"
- "Seeing my growth over time motivates me to keep improving"
- "The tips are actually actionable, not generic advice"
- "I can tell I'm getting better at breaking down problems for agents"

**Red Flags:**
- "This feels like surveillance"
- "The metrics don't make sense to me"
- "I don't know what to do with this information"
- "I checked once and never came back"

---

## V2 Metrics: Team Insights

**Phase Goal:** Prove that team transparency drives peer learning and collective improvement.

### Leading Indicators

**Team Adoption:**
- **Team participation rate** - 70%+ of team members have V1 installed and active
- **Peer comparison engagement** - 50%+ team members view team metrics weekly
- **Knowledge sharing** - 3+ instances per month of "what do top performers do differently" queries

**Collaboration:**
- **Cross-team learning** - 40%+ users report trying techniques they saw from peer metrics
- **Healthy competition** - Team members reference percentile rankings in positive context

### Lagging Indicators

**Team Performance:**
- **Collective skill growth** - Team's average AI effectiveness score increases 20%+ over 90 days
- **Skill distribution** - Gap between top and bottom performers narrows over time (learning spreading)
- **Best practice adoption** - When high-performer patterns are identified, 60%+ of team tries them within 2 weeks

**Team Culture:**
- **Transparency acceptance** - 80%+ team members support keeping metrics public
- **Peer mentoring** - 5+ peer coaching conversations per month driven by insights
- **Team health** - Overall engagement and satisfaction with AI tools increases

### Qualitative Signals

**User Quotes We Listen For:**
- "I saw that top performers use grep more—I tried it and it's way faster"
- "The team metrics help us learn from each other instead of competing negatively"
- "Knowing where I stand motivates me to level up"

**Red Flags:**
- "This feels like a ranking system, not a learning tool"
- "Team metrics create pressure, not growth"
- "I don't want my peers seeing my performance"

---

## V3 Metrics: Manager Insights

**Phase Goal:** Demonstrate that managers can build world-class AI-first teams through data-driven coaching.

### Leading Indicators

**Manager Engagement:**
- **Weekly dashboard usage** - 80%+ managers check team health weekly
- **Coaching conversations** - 3+ coaching sessions per month triggered by insights
- **Development planning** - 90%+ managers incorporate AI skill development in 1-on-1s

**Proactive Intervention:**
- **Early detection** - 70%+ of struggling team members identified before outcomes degrade
- **Targeted coaching** - 80%+ of coaching conversations reference specific patterns from insights
- **Development plans** - 60%+ team members have AI skill development goals

### Lagging Indicators

**Team Capability:**
- **World-class benchmark** - Team's collective AI effectiveness in top 25% of measured teams
- **Reduced skill variance** - Standard deviation of team skill scores decreases over 6 months
- **Sustained growth** - 90%+ team members show continuous improvement over 6-month period

**Manager Effectiveness:**
- **Team retention** - High performers stay (>90% retention of top quartile)
- **Skill development velocity** - Average time from "struggling" to "proficient" decreases
- **Manager satisfaction** - 80%+ managers feel confident coaching AI skills with data

### Qualitative Signals

**User Quotes We Listen For:**
- "I can finally coach my team on AI skills with data, not gut feel"
- "Early warning signals help me help team members before they struggle"
- "I can see who's ready for more complex work vs. who needs more support"

**Red Flags:**
- "This feels like performance review ammunition"
- "I don't trust the metrics to assess my team"
- "The insights don't translate to actionable coaching"

---

## User Satisfaction Signals

### Quantitative

**Net Promoter Score (NPS):**
- Target: 40+ for V1, 50+ for V2, 60+ for V3
- Segment by: Individual Contributors, Team Members, Managers

**Feature Satisfaction (1-5 scale):**
- **Accuracy**: "Do the insights match your experience?" - Target: 4.0+
- **Actionability**: "Do insights help you improve?" - Target: 4.2+
- **Fairness**: "Are metrics fair and account for context?" - Target: 4.0+
- **Usefulness**: "Would you miss this if it went away?" - Target: 4.5+

### Qualitative

**Validation Statements:**
- "This helped me level up my AI skills"
- "I didn't know I had these patterns until I saw the data"
- "The team uses insights to learn from each other"
- "Coaching conversations are way more effective with this data"

**Concern Statements:**
- "I don't understand what this metric means"
- "This feels like Big Brother watching me"
- "The insights don't help me improve"
- "I tried the tips but they didn't work"

---

## What We Don't Measure

**Intentionally excluded metrics that would distort behavior:**

### ❌ Total Time Spent in AI Sessions

**Why not:** Encourages staying in sessions longer instead of being effective. We want efficiency, not hours logged. Long sessions might indicate struggle, not productivity.

### ❌ Absolute Session Count

**Why not:** Encourages quantity over quality. Someone solving hard problems in 3 sessions is more effective than someone doing simple tasks in 30 sessions.

### ❌ Individual Rankings or Scores

**Why not:** Creates competitive pressure instead of growth mindset. We show percentiles and comparisons, but not "Employee #1, #2, #3" rankings that encourage gaming the system.

### ❌ Real-Time Activity Monitoring

**Why not:** This is surveillance, not growth. Managers don't need to see "who's working right now"—they need to see patterns over time and coaching opportunities.

### ❌ Outcome-Only Metrics (Features Shipped, Business Impact)

**Why not:** These are important but insufficient. They miss process quality, growth, and fair assessment. We complement outcomes with leading indicators.

### ❌ Comparison to External Benchmarks (Yet)

**Why not:** In V1-V3, we compare to team/self, not to "industry standard" (doesn't exist yet for AI-first work). External benchmarks come after we validate internal improvement works.

---

## Success Criteria by Phase

### V1 Success = Individuals Choose to Use This

**Must achieve:**
- **50%+ weekly active usage** - Installed users check insights at least once per week
- **60%+ find value** - Users rate insights as valuable or very valuable
- **40%+ behavior change** - Users report changing how they work with AI based on insights
- **50%+ retention at 60 days** - Still using 2 months after install

**Qualitative validation:**
- 10+ unprompted testimonials about improvement
- Users proactively share insights with teammates
- Feature requests indicate deepening engagement (want more, not less)

**Strategic validation:**
- Proof that **measurement drives improvement** (users with insights improve faster than control group)
- Insights on **which metrics users care about most** (guides V2 priorities)
- Understanding of **what tips are most actionable** (refine improvement guidance)

### V2 Success = Teams Learn Together

**Must achieve:**
- **70%+ team participation** - Majority of team members actively using
- **60%+ peer learning** - Team members report learning from peer metrics
- **30%+ team skill growth** - Collective AI effectiveness increases measurably
- **80%+ support transparency** - Team accepts public metrics culture

**Qualitative validation:**
- Teams reference insights in standups or retrospectives
- Peer mentoring happens organically based on identified patterns
- Team culture shifts to value AI skill development

**Strategic validation:**
- Proof that **transparency drives collective growth** (transparent teams improve faster than opaque teams)
- Insights on **what team dynamics emerge** (competitive vs. collaborative)
- Understanding of **resistance points** (what makes teams uncomfortable)

### V3 Success = Managers Build World-Class Teams

**Must achieve:**
- **80%+ manager engagement** - Managers use insights weekly for coaching
- **Top 25% capability** - Teams rank in top quartile of AI effectiveness
- **70%+ proactive coaching** - Struggling team members identified early (before outcomes degrade)
- **90%+ team member improvement** - Continuous growth over 6-month period

**Qualitative validation:**
- Managers cite insights in coaching conversations regularly
- Team members report receiving better, more targeted coaching
- Organizations invest in AI skill development based on insights

**Strategic validation:**
- Proof that **data-driven coaching beats intuition-based coaching** (teams with manager insights grow faster)
- Insights on **what makes an effective AI-first manager** (identify coaching patterns)
- Understanding of **team capability ceiling** (limits of improvement through measurement alone)

---

## Measuring What Matters

### The Questions We Keep Asking

**After every user interaction:**
- Did they get a clear, actionable insight?
- Did they return for more?
- Did they change behavior based on what they learned?

**After every feature:**
- Does this help users improve AI collaboration skills?
- Does this align with growth over surveillance?
- Does this follow the Individual → Team → Manager sequence?

**After every month:**
- Are users measurably improving?
- Are we learning which metrics matter most?
- Are we on track to build world-class AI-first teams?

### The Standard

**"If a metric doesn't help answer 'are users getting better at AI collaboration?', we don't track it."**

We are ruthlessly focused on improvement. Vanity metrics (total users, total sessions, time spent) might look good but don't tell us if we're achieving the vision. We measure:
- **Behavior change** - Are users working differently?
- **Skill growth** - Are users improving over time?
- **Value creation** - Are insights actionable and acted upon?
- **Strategic validation** - Are we proving our hypotheses about AI-first teams?

Everything else is noise.

---

## Related Documentation

**Vision folder (strategic context):**
- [VISION.md](VISION.md) - Strategic vision and positioning
- [PRINCIPLES.md](PRINCIPLES.md) - Implementation philosophy

**Implementation details:**
- [docs/README.md](../README.md) - Epic index
- [Epics](../02-requirements/epics/) - Feature requirements

---

## Change History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| v1.0 | 2026-02-03 | Chris Park | Initial success metrics document |

---
